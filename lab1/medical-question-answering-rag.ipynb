{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b985c5b5-1d65-4e4a-82d1-2dfc9768d97d",
   "metadata": {},
   "source": [
    "## Medical Question answering with Retrieval Augmented Generation design pattern. \n",
    "Use Python 3 (Data Science 3.0) kernel image and `ml.m5.2xlarge` for this notebook.\n",
    "\n",
    "This includes generating embeddings of all existing documents, indexing them in a vector store. Then for every user query, generate local embeddings and search based on embedding distance. The search responses act as context to the LLM model to generate a output. \n",
    "\n",
    "Challenges:\n",
    "How to manage large document(s) that exceed the token limitHow to find the document(s) relevant to the question being asked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84657b-0fff-4476-980d-d74894a74c27",
   "metadata": {},
   "source": [
    "## Key components\n",
    "\n",
    "LLM (Large Language Model): Falcon-40b-instruct available through Amazon SageMaker This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "\n",
    "Embeddings Model: GPT-J 6B available through Amazon SageMaker. This model will be used to generate a numerical representation of the textual documents.\n",
    "\n",
    "Vector Store: FAISS available through LangChainIn this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "\n",
    "Index: VectorIndex The index helps to compare the input embedding and the document embeddings to find relevant document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87a992-1790-4d4e-8ffd-ab7c053156c1",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "To explain this architecture pattern we are using the documents from MedQA. These documents include medical textbooks such as:\n",
    "Pathology, Anatomy, Pharmacology and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c4a8b-f1a1-4fc6-831c-3fdb522cf941",
   "metadata": {},
   "source": [
    "Download textbooks that are part of Q&A dataset MedQA released as part of Jin, Di, et al. \"What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams.\" arXiv preprint arXiv:2009.13081 (2020). \n",
    "\n",
    "More details are available here https://github.com/jind11/MedQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ed552-d458-4dd3-9b13-e3e997e10ade",
   "metadata": {},
   "source": [
    "* Data source : @article{jin2020disease,\n",
    "  title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},\n",
    "  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},\n",
    "  journal={arXiv preprint arXiv:2009.13081},\n",
    "  year={2020} }\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff5221-0349-4617-814d-98c79db7cfb1",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "> **NOTICE**: \"This link leads to a Third-Party Dataset. AWS does not own, nor does it have any control over the Third-Party Dataset. You should perform your own independent assessment, and take measures to ensure that you comply with your own specific quality control practices and standards, and the local rules, laws, regulations, licenses and terms of use that apply to you, your content, and the Third-Party Dataset. AWS does not make any representations or warranties that the Third-Party Dataset is secure, virus-free, accurate, operational, or compatible with your own environment and standards. AWS does not make any representations, warranties or guarantees that any information in the Third-Party Dataset will result in a particular outcome or result.\"\n",
    "\n",
    "1. The full dataset can be downloaded can be seen from https://drive.google.com/file/d/1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw/view?usp=sharing. You can read more about the dataset in https://github.com/jind11/MedQA#data.\n",
    "2. To speed up the uploading for this lab, a smaller version of dataset is already downloaded -  https://d2qrbbbqnxtln.cloudfront.net/Pathology_Robbins.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa4ada-a686-43de-bc90-0f4107f95ce1",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b71850-cf8d-4232-866e-53f7a8a685e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install faiss-cpu==1.7.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139ec9e-962f-44b4-94c1-9a778e4d2be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.222 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf12933-564a-41a3-9a9e-c02a9437310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install PyYAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52706ec1-fd47-42d4-af0f-33f0a03f654d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eea5f6-50e3-4398-80eb-11b680d026b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging \n",
    "import boto3\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec6d42-db7a-4c4c-8322-95963806f987",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddd51c-9251-4428-9931-b2700a71142a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81f78f-675d-4009-9228-8fa3cfc559b8",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75474334-8b19-4491-abb9-58468bc33329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f'Using requests=={requests.__version__}')\n",
    "logger.info(f'Using pyyaml=={yaml.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e3b3f-f770-4b90-bc35-12cc0f793604",
   "metadata": {},
   "source": [
    "#### Setup essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a24c9-b504-45a0-888d-fe507b34402d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ENDPOINT_NAME = 'jumpstart-dft-hf-textembedding-gpt-j-6b-fp16' #INSERT EMBEDDING ENDPOINT NAME IF DIFFERENT\n",
    "TEXT_GENERATION_MODEL_ENDPOINT_NAME = 'jumpstart-dft-hf-llm-falcon-7b-instruct-bf16' #INSERT TEXT GENERATION ENDPOINT NAME IF DIFFERENT\n",
    "\n",
    "REGION_NAME = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15360977-8631-4a21-a599-b91c91c8c893",
   "metadata": {},
   "source": [
    "#### Encode passages (chunks) using JumpStart's GPT-J text embedding model . We are specifically using only 1 of 20 textbooks from the dataset. It takes about 6 minutes to generate embeddings for one textbook (for example, Pathology). You can increase the number of textbooks indexed by adding sufficient time buffer for execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ed59e-7553-4e57-af5e-29d345a00028",
   "metadata": {},
   "source": [
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7c4b7-c6bd-40c2-b91e-0d6370fce251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./\", glob=\"**/Pathology*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63c24e-190f-46ef-bfc7-2dac1112765e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d6873-3ea9-4455-8b8d-cad845009a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82c416-9401-4b81-af41-f1ff334902d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sagemakerEndpointEmbeddingsJumpStart = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=TEXT_EMBEDDING_MODEL_ENDPOINT_NAME,\n",
    "    region_name=REGION_NAME,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce0559-7ac9-429d-8527-6617759d3249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bdba3-8541-495b-8304-c22e4fa05ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Semantic Similarity with Amazon Jumpstart Embedding Models\n",
    "\n",
    "Semantic search refers to searching for information based on the meaning and concepts of words and phrases, rather than just matching keywords. Embedding models like Amazon Titan Embeddings allow semantic search by representing words and sentences as dense vectors that encode their semantic meaning.\n",
    "\n",
    "Semantic matching is extremely helpful for RAG because it returns results that are conceptually related to the user's query, even if they don't contain the exact keywords. This leads to more relevant and useful search results which can be injected into our LLM's prompts.\n",
    "\n",
    "First, let's take a look below to illustrate the sample of an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f2c08-3ba7-4e2b-8c70-179b794863b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_embedding = np.array(sagemakerEndpointEmbeddingsJumpStart.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c7292-6931-4155-9e23-5c546801e868",
   "metadata": {},
   "source": [
    "Now create embeddings for the entire document set. Note for a single medical textbook, it takes about 6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe662107-0401-42a5-8e02-b9045fb2a466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def generate_embeddings(x):\n",
    "    return (x, sagemakerEndpointEmbeddingsJumpStart.embed_query(x))\n",
    "    \n",
    "workers = 1 * cpu_count()\n",
    "\n",
    "texts = [i.page_content for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba56c8f-be28-4268-b839-3aef2ec6d755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9badfc-2ca9-409e-acf9-416ddb294291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = process_map(generate_embeddings, texts, max_workers=workers, chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606ae3c-3ed2-4cd1-b9ba-40088614e0fa",
   "metadata": {},
   "source": [
    "Next, we insert the embeddings to the FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462eaea-7c73-417e-afe1-dcf0b7cf3026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "faiss = FAISS.from_documents(docs[0:2], sagemakerEndpointEmbeddingsJumpStart)\n",
    "faiss.add_embeddings(data)\n",
    "faiss.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fef5b-06cb-401a-812d-f4f1823398e9",
   "metadata": {},
   "source": [
    "Next we create user query to retrieve a response from vector search and LLM combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a412e-c6eb-4e68-a334-5318d503f8c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is acute kidney injury?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96620221-20d4-4559-990e-f9b2c4c9c1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embedding = faiss.embedding_function(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2d2ef-5c20-47a1-98c1-d789ee0452fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_documents = faiss.similarity_search_by_vector(query_embedding)\n",
    "context = \"\"\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')\n",
    "    context += rel_doc.page_content\n",
    "context = context.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e423c-19d2-4d62-8271-332038785c1d",
   "metadata": {},
   "source": [
    "Now create a prompt template to trigger the model with above context from vector search. We specifically inform the model to answer only using the context provied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ecdbd-e4c5-4a1b-8db9-0d82390c6a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are a helpful, polite, fact-based agent.\n",
    "        If you don't know the answer, just say that you don't know.\n",
    "        Please answer the following question using the context provided. \n",
    "\n",
    "        CONTEXT: \n",
    "        {context}\n",
    "        =========\n",
    "        QUESTION: {question} \n",
    "        ANSWER: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31b73a-cfaa-4d9c-915c-ca799c3fc0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(context=context, question=query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d41b64-49cf-4842-82f6-286bee678e3d",
   "metadata": {},
   "source": [
    "Invoke the endpoint to generate a response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678113c4-4c76-49ad-8ccb-095ad9a27529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ae283-0745-4849-b832-1a735bf06799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=TEXT_GENERATION_MODEL_ENDPOINT_NAME,\n",
    "    Body=json.dumps(\n",
    "        {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 500}}\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "response = json.loads(response_model[\"Body\"].read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113187c-17a2-49a7-9443-e28e27934bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
